{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Master Dataset Pipeline\n",
        "## Processing 14 Years of Toronto Open Data (2010-2024)\n",
        "\n",
        "This notebook processes all CSV files from `data/raw_data` into a unified longitudinal dataset optimized for XGBoost time-series forecasting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setup complete!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set paths (notebook is in data/ directory)\n",
        "RAW_DATA_DIR = Path('raw_data')\n",
        "PROCESSED_DATA_DIR = Path('processed_data')\n",
        "PROCESSED_DATA_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "print(\"Setup complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Data Ingestion & Schema Alignment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 15 CSV files to process\n",
            "\n",
            "Processing 2010...\n",
            "  Loaded 158 rows\n",
            "\n",
            "Processing 2011...\n",
            "  Loaded 158 rows\n",
            "\n",
            "Processing 2012...\n",
            "  Loaded 158 rows\n",
            "\n",
            "Processing 2013...\n",
            "  Loaded 158 rows\n",
            "\n",
            "Processing 2014...\n",
            "  Loaded 158 rows\n",
            "\n",
            "Processing 2015...\n",
            "  Loaded 158 rows\n",
            "\n",
            "Processing 2016...\n",
            "  Loaded 158 rows\n",
            "\n",
            "Processing 2017...\n",
            "  Loaded 317 rows\n",
            "\n",
            "Processing 2018...\n",
            "  Loaded 158 rows\n",
            "\n",
            "Processing 2019...\n",
            "  Loaded 158 rows\n",
            "\n",
            "Processing 2020...\n",
            "  Loaded 158 rows\n",
            "\n",
            "Processing 2021...\n",
            "  Loaded 158 rows\n",
            "\n",
            "Processing 2022...\n",
            "  Loaded 158 rows\n",
            "\n",
            "Processing 2023...\n",
            "  Loaded 158 rows\n",
            "\n",
            "Processing 2024...\n",
            "  Loaded 158 rows\n",
            "\n",
            "\n",
            "Total years processed: 15\n",
            "Years: [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024]\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Loop through all CSV files and standardize\n",
        "all_dataframes = []\n",
        "years_processed = []\n",
        "\n",
        "# Get all CSV files matching the pattern 20XX.csv\n",
        "csv_files = sorted([f for f in RAW_DATA_DIR.glob('20*.csv')])\n",
        "print(f\"Found {len(csv_files)} CSV files to process\")\n",
        "\n",
        "for csv_file in csv_files:\n",
        "    # Extract year from filename\n",
        "    year_match = re.search(r'(\\d{4})\\.csv$', csv_file.name)\n",
        "    if not year_match:\n",
        "        print(f\"Warning: Could not extract year from {csv_file.name}, skipping\")\n",
        "        continue\n",
        "    \n",
        "    year = int(year_match.group(1))\n",
        "    print(f\"\\nProcessing {year}...\")\n",
        "    \n",
        "    try:\n",
        "        df = pd.read_csv(csv_file, low_memory=False)\n",
        "        \n",
        "        # Standardize AREA_NAME column\n",
        "        # Find the area name column (could be AREA_NAME, Area, or similar)\n",
        "        area_col = None\n",
        "        for col in df.columns:\n",
        "            if 'area' in col.lower() and 'name' in col.lower():\n",
        "                area_col = col\n",
        "                break\n",
        "        \n",
        "        if area_col and area_col != 'AREA_NAME':\n",
        "            df = df.rename(columns={area_col: 'AREA_NAME'})\n",
        "        \n",
        "        if 'AREA_NAME' not in df.columns:\n",
        "            print(f\"  Warning: No AREA_NAME column found in {year}, using first column\")\n",
        "            df = df.rename(columns={df.columns[0]: 'AREA_NAME'})\n",
        "        \n",
        "        # Clean AREA_NAME: strip whitespace and convert to consistent casing\n",
        "        df['AREA_NAME'] = df['AREA_NAME'].astype(str).str.strip().str.title()\n",
        "        \n",
        "        # Remove garbage rows (rows containing '=======' or similar metadata)\n",
        "        mask = df['AREA_NAME'].str.contains('={3,}', na=False, regex=True)\n",
        "        df = df[~mask]\n",
        "        \n",
        "        # Remove rows where AREA_NAME is empty or NaN\n",
        "        df = df[df['AREA_NAME'].notna() & (df['AREA_NAME'] != '')]\n",
        "        \n",
        "        # Add YEAR column\n",
        "        df['YEAR'] = year\n",
        "        \n",
        "        all_dataframes.append(df)\n",
        "        years_processed.append(year)\n",
        "        print(f\"  Loaded {len(df)} rows\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"  Error processing {year}: {e}\")\n",
        "        continue\n",
        "\n",
        "print(f\"\\n\\nTotal years processed: {len(years_processed)}\")\n",
        "print(f\"Years: {sorted(years_processed)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Normalized 15 dataframes\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Strip year suffixes from columns and normalize\n",
        "normalized_dfs = []\n",
        "\n",
        "for df in all_dataframes:\n",
        "    year = df['YEAR'].iloc[0]\n",
        "    df_normalized = df.copy()\n",
        "    \n",
        "    # Create mapping for column renaming\n",
        "    rename_dict = {}\n",
        "    \n",
        "    for col in df.columns:\n",
        "        # Skip YEAR and AREA_NAME columns\n",
        "        if col in ['YEAR', 'AREA_NAME']:\n",
        "            continue\n",
        "        \n",
        "        # Check if column ends with _20XX pattern\n",
        "        year_pattern = re.search(r'_(\\d{4})$', col)\n",
        "        if year_pattern:\n",
        "            # Strip the year suffix\n",
        "            new_col = col[:-5]  # Remove '_YYYY'\n",
        "            rename_dict[col] = new_col\n",
        "    \n",
        "    # Apply renaming\n",
        "    df_normalized = df_normalized.rename(columns=rename_dict)\n",
        "    \n",
        "    normalized_dfs.append(df_normalized)\n",
        "\n",
        "print(f\"Normalized {len(normalized_dfs)} dataframes\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Advanced Rent Feature Engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "After Step 3, column count check:\n",
            "Sample dataframe has 43 columns\n",
            "Columns: ['AREA_NAME', 'CLASSIFICATION', 'CLASSIFICATION_CODE', 'geometry_wkt', 'geometry_type', 'Area', 'bachelor_avg_lease_rate', '1_bedrooms_leased', '1_bed_room_avg_lease_rate', '2_bedrooms_leased', '2_bedrooms_avg_lease_rate', '3_bedrooms_leased', '3_bedrooms_avg_lease_rate', 'area_sq_meters', 'perimeter_meters']...\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Rent feature engineering\n",
        "rent_engineered_dfs = []\n",
        "\n",
        "for df in normalized_dfs:\n",
        "    df_rent = df.copy()\n",
        "    \n",
        "    # Find 1-BR rent columns that might be split by quarter\n",
        "    # Look for columns like: 1_bedroom_avg_lease_rate_q1, q2, q3, q4\n",
        "    q1_cols = [col for col in df_rent.columns if '1' in col.lower() and 'bedroom' in col.lower() \n",
        "               and 'avg' in col.lower() and ('lease' in col.lower() or 'rent' in col.lower())\n",
        "               and ('q1' in col.lower() or 'quarter' in col.lower())]\n",
        "    \n",
        "    # Also check for standard naming: 1_bed_room_avg_lease_rate, 1_bedrooms_avg_lease_rate\n",
        "    standard_1br_cols = [col for col in df_rent.columns if \n",
        "                        (('1_bed' in col.lower() or '1bed' in col.lower()) \n",
        "                         and ('avg' in col.lower() or 'average' in col.lower())\n",
        "                         and ('lease' in col.lower() or 'rent' in col.lower()))]\n",
        "    \n",
        "    # If we have quarterly columns, aggregate them\n",
        "    if q1_cols:\n",
        "        # Find all quarter columns\n",
        "        quarter_cols = []\n",
        "        for q in ['q1', 'q2', 'q3', 'q4']:\n",
        "            q_cols = [col for col in df_rent.columns if q in col.lower() and '1' in col.lower() \n",
        "                     and 'bedroom' in col.lower() and 'avg' in col.lower()]\n",
        "            quarter_cols.extend(q_cols)\n",
        "        \n",
        "        if quarter_cols:\n",
        "            # Calculate row-wise mean\n",
        "            df_rent['avg_rent_1br'] = df_rent[quarter_cols].mean(axis=1)\n",
        "        else:\n",
        "            # Use standard column if available\n",
        "            if standard_1br_cols:\n",
        "                df_rent['avg_rent_1br'] = df_rent[standard_1br_cols[0]]\n",
        "            else:\n",
        "                df_rent['avg_rent_1br'] = np.nan\n",
        "    else:\n",
        "        # Use standard column naming\n",
        "        if standard_1br_cols:\n",
        "            df_rent['avg_rent_1br'] = df_rent[standard_1br_cols[0]]\n",
        "        else:\n",
        "            # Try alternative naming patterns\n",
        "            alt_cols = [col for col in df_rent.columns if '1' in str(col) and 'bed' in str(col).lower() \n",
        "                       and ('rate' in str(col).lower() or 'rent' in str(col).lower())]\n",
        "            if alt_cols:\n",
        "                df_rent['avg_rent_1br'] = df_rent[alt_cols[0]]\n",
        "            else:\n",
        "                df_rent['avg_rent_1br'] = np.nan\n",
        "    \n",
        "    # Keep other rent types\n",
        "    # Bachelor rent\n",
        "    bachelor_cols = [col for col in df_rent.columns if 'bachelor' in col.lower() \n",
        "                    and ('avg' in col.lower() or 'average' in col.lower())\n",
        "                    and ('lease' in col.lower() or 'rent' in col.lower())]\n",
        "    if bachelor_cols:\n",
        "        df_rent['bachelor_avg_lease_rate'] = df_rent[bachelor_cols[0]]\n",
        "    \n",
        "    # 2-bedroom rent\n",
        "    two_br_cols = [col for col in df_rent.columns if '2' in col and 'bedroom' in col.lower() \n",
        "                  and ('avg' in col.lower() or 'average' in col.lower())\n",
        "                  and ('lease' in col.lower() or 'rent' in col.lower())]\n",
        "    if two_br_cols:\n",
        "        df_rent['2_bedrooms_avg_lease_rate'] = df_rent[two_br_cols[0]]\n",
        "    \n",
        "    # Volume tracking: 1_bedrooms_leased\n",
        "    leased_cols = [col for col in df_rent.columns if '1' in col and 'bedroom' in col.lower() \n",
        "                  and 'leased' in col.lower()]\n",
        "    if leased_cols:\n",
        "        df_rent['1_bedrooms_leased'] = df_rent[leased_cols[0]]\n",
        "    \n",
        "    rent_engineered_dfs.append(df_rent)\n",
        "\n",
        "print(f\"\\nAfter Step 3, column count check:\")\n",
        "if rent_engineered_dfs:\n",
        "    print(f\"Sample dataframe has {len(rent_engineered_dfs[0].columns)} columns\")\n",
        "    print(f\"Columns: {list(rent_engineered_dfs[0].columns)[:15]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Population and Geography Proxying\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Static columns mapping: {'POPULATION': 'POPULATION', 'area_sq_meters': 'area_sq_meters', 'transit_line_density': 'transit_line_density', 'avg_stop_frequency': 'avg_stop_frequency', 'distinct_route_count': 'distinct_route_count'}\n",
            "\n",
            "Broadcasted static geography data to 15 dataframes\n"
          ]
        }
      ],
      "source": [
        "# Step 4: Use 2024 as source of truth for static geographic data\n",
        "# Find 2024 dataframe\n",
        "df_2024 = None\n",
        "for df in rent_engineered_dfs:\n",
        "    if df['YEAR'].iloc[0] == 2024:\n",
        "        df_2024 = df.copy()\n",
        "        break\n",
        "\n",
        "if df_2024 is None:\n",
        "    raise ValueError(\"2024 dataset not found!\")\n",
        "\n",
        "# Extract static geographic columns from 2024\n",
        "static_cols = ['POPULATION', 'area_sq_meters', 'transit_line_density', \n",
        "               'avg_stop_frequency', 'distinct_route_count']\n",
        "\n",
        "# Find actual column names (may have variations)\n",
        "static_mapping = {}\n",
        "for target_col in static_cols:\n",
        "    # Try exact match first\n",
        "    if target_col in df_2024.columns:\n",
        "        static_mapping[target_col] = target_col\n",
        "    else:\n",
        "        # Try case-insensitive and partial matches\n",
        "        matches = [col for col in df_2024.columns \n",
        "                 if target_col.lower() in col.lower() or col.lower() in target_col.lower()]\n",
        "        if matches:\n",
        "            static_mapping[target_col] = matches[0]\n",
        "        else:\n",
        "            print(f\"Warning: Could not find column matching '{target_col}'\")\n",
        "\n",
        "print(f\"Static columns mapping: {static_mapping}\")\n",
        "\n",
        "# Create lookup dictionary: AREA_NAME -> static values\n",
        "static_lookup = {}\n",
        "for idx, row in df_2024.iterrows():\n",
        "    area_name = row['AREA_NAME']\n",
        "    static_lookup[area_name] = {}\n",
        "    for target_col, source_col in static_mapping.items():\n",
        "        static_lookup[area_name][target_col] = row[source_col]\n",
        "\n",
        "# Broadcast 2024 values to all previous years\n",
        "geography_proxied_dfs = []\n",
        "for df in rent_engineered_dfs:\n",
        "    df_geo = df.copy()\n",
        "    \n",
        "    # Add static columns\n",
        "    for target_col in static_cols:\n",
        "        if target_col not in df_geo.columns:\n",
        "            df_geo[target_col] = df_geo['AREA_NAME'].map(\n",
        "                lambda x: static_lookup.get(x, {}).get(target_col, np.nan) if pd.notna(x) else np.nan\n",
        "            )\n",
        "    \n",
        "    geography_proxied_dfs.append(df_geo)\n",
        "\n",
        "print(f\"\\nBroadcasted static geography data to {len(geography_proxied_dfs)} dataframes\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Temporal Feature Engineering (The \"Memory\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Combined dataframe shape: (2529, 93)\n",
            "Unique neighborhoods: 159\n",
            "Year range: 2010 - 2024\n",
            "\n",
            "Temporal features created:\n",
            "  - rent_lag_1: 2370 non-null values\n",
            "  - rent_lag_2: 2212 non-null values\n",
            "  - rent_growth_rate: 2370 non-null values\n"
          ]
        }
      ],
      "source": [
        "# Step 5: Combine all dataframes and create temporal features\n",
        "# Combine all dataframes\n",
        "combined_df = pd.concat(geography_proxied_dfs, ignore_index=True)\n",
        "\n",
        "# Sort by AREA_NAME and YEAR\n",
        "combined_df = combined_df.sort_values(['AREA_NAME', 'YEAR']).reset_index(drop=True)\n",
        "\n",
        "print(f\"Combined dataframe shape: {combined_df.shape}\")\n",
        "print(f\"Unique neighborhoods: {combined_df['AREA_NAME'].nunique()}\")\n",
        "print(f\"Year range: {combined_df['YEAR'].min()} - {combined_df['YEAR'].max()}\")\n",
        "\n",
        "# Create lag features grouped by AREA_NAME\n",
        "combined_df['rent_lag_1'] = combined_df.groupby('AREA_NAME')['avg_rent_1br'].shift(1)\n",
        "combined_df['rent_lag_2'] = combined_df.groupby('AREA_NAME')['avg_rent_1br'].shift(2)\n",
        "\n",
        "# Calculate rent growth rate\n",
        "combined_df['rent_growth_rate'] = (\n",
        "    (combined_df['avg_rent_1br'] - combined_df['rent_lag_1']) / combined_df['rent_lag_1'] * 100\n",
        ")\n",
        "# Handle division by zero\n",
        "combined_df['rent_growth_rate'] = combined_df['rent_growth_rate'].replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "print(f\"\\nTemporal features created:\")\n",
        "print(f\"  - rent_lag_1: {combined_df['rent_lag_1'].notna().sum()} non-null values\")\n",
        "print(f\"  - rent_lag_2: {combined_df['rent_lag_2'].notna().sum()} non-null values\")\n",
        "print(f\"  - rent_growth_rate: {combined_df['rent_growth_rate'].notna().sum()} non-null values\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Creating the Machine Learning Label (The \"Answer\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Verification - Sample 2012 rows:\n",
            "                       AREA_NAME  YEAR  avg_rent_1br  TARGET_RENT_5YR\n",
            "2                Agincourt North  2012       1272.75      1644.833333\n",
            "18  Agincourt South-Malvern West  2012       1272.75      1644.833333\n",
            "34                     Alderwood  2012       1520.50      2141.166667\n",
            "50                         Annex  2012       2045.75      3742.000000\n",
            "66                      Avondale  2012       1490.00      2175.000000\n",
            "\n",
            "Verification - Sample 2017 rows (should match TARGET_RENT_5YR from 2012):\n",
            "Area: Agincourt North\n",
            "2012 TARGET_RENT_5YR: 1644.8333333333333\n",
            "2017 avg_rent_1br: 1644.8333333333333\n",
            "Match: True\n",
            "\n",
            "TARGET_RENT_5YR statistics:\n",
            "  Non-null values: 1738\n",
            "  Null values: 791\n"
          ]
        }
      ],
      "source": [
        "# Step 6: Create TARGET_RENT_5YR column\n",
        "# For each row, find the avg_rent_1br value from 5 years later\n",
        "\n",
        "# Create a lookup: (AREA_NAME, YEAR) -> avg_rent_1br\n",
        "rent_lookup = {}\n",
        "for idx, row in combined_df.iterrows():\n",
        "    key = (row['AREA_NAME'], row['YEAR'])\n",
        "    rent_lookup[key] = row['avg_rent_1br']\n",
        "\n",
        "# Create TARGET_RENT_5YR\n",
        "combined_df['TARGET_RENT_5YR'] = combined_df.apply(\n",
        "    lambda row: rent_lookup.get((row['AREA_NAME'], row['YEAR'] + 5), np.nan),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Verification: Check a 2012 row shows 2017 rent\n",
        "print(\"\\nVerification - Sample 2012 rows:\")\n",
        "sample_2012 = combined_df[combined_df['YEAR'] == 2012][['AREA_NAME', 'YEAR', 'avg_rent_1br', 'TARGET_RENT_5YR']].head()\n",
        "print(sample_2012)\n",
        "\n",
        "print(\"\\nVerification - Sample 2017 rows (should match TARGET_RENT_5YR from 2012):\")\n",
        "if len(sample_2012) > 0:\n",
        "    test_area = sample_2012.iloc[0]['AREA_NAME']\n",
        "    sample_2017 = combined_df[(combined_df['YEAR'] == 2017) & (combined_df['AREA_NAME'] == test_area)]\n",
        "    if len(sample_2017) > 0:\n",
        "        print(f\"Area: {test_area}\")\n",
        "        print(f\"2012 TARGET_RENT_5YR: {sample_2012.iloc[0]['TARGET_RENT_5YR']}\")\n",
        "        print(f\"2017 avg_rent_1br: {sample_2017.iloc[0]['avg_rent_1br']}\")\n",
        "        print(f\"Match: {abs(sample_2012.iloc[0]['TARGET_RENT_5YR'] - sample_2017.iloc[0]['avg_rent_1br']) < 0.01 if pd.notna(sample_2012.iloc[0]['TARGET_RENT_5YR']) else 'N/A (NaN)'}\")\n",
        "\n",
        "print(f\"\\nTARGET_RENT_5YR statistics:\")\n",
        "print(f\"  Non-null values: {combined_df['TARGET_RENT_5YR'].notna().sum()}\")\n",
        "print(f\"  Null values: {combined_df['TARGET_RENT_5YR'].isna().sum()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Separation for GIS (Map Key)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Map key dataframe shape: (158, 4)\n",
            "Map key columns: ['AREA_NAME', 'geometry_wkt', 'geometry_type', 'CLASSIFICATION']\n",
            "\n",
            "Dropped geometry columns from main dataframe\n",
            "Main dataframe shape after dropping geometry: (2529, 95)\n"
          ]
        }
      ],
      "source": [
        "# Step 7: Extract geometry columns for map key\n",
        "geometry_cols = ['AREA_NAME', 'geometry_wkt', 'geometry_type', 'CLASSIFICATION']\n",
        "\n",
        "# Get geometry data from 2024 (most complete)\n",
        "map_key_df = df_2024[geometry_cols].copy() if all(col in df_2024.columns for col in geometry_cols) else pd.DataFrame()\n",
        "\n",
        "# If 2024 doesn't have all columns, try to get from combined_df\n",
        "if map_key_df.empty or len(map_key_df) == 0:\n",
        "    available_geo_cols = [col for col in geometry_cols if col in combined_df.columns]\n",
        "    if available_geo_cols:\n",
        "        # Get unique combinations (prefer latest year)\n",
        "        map_key_df = combined_df.sort_values('YEAR', ascending=False).groupby('AREA_NAME').first()[available_geo_cols].reset_index()\n",
        "\n",
        "# Ensure AREA_NAME is in the result\n",
        "if 'AREA_NAME' not in map_key_df.columns and len(map_key_df) > 0:\n",
        "    map_key_df = map_key_df.reset_index()\n",
        "\n",
        "print(f\"Map key dataframe shape: {map_key_df.shape}\")\n",
        "print(f\"Map key columns: {list(map_key_df.columns)}\")\n",
        "\n",
        "# Drop geometry columns from main training dataframe\n",
        "geometry_cols_to_drop = ['geometry_wkt', 'geometry_type']\n",
        "for col in geometry_cols_to_drop:\n",
        "    if col in combined_df.columns:\n",
        "        combined_df = combined_df.drop(columns=[col])\n",
        "\n",
        "print(f\"\\nDropped geometry columns from main dataframe\")\n",
        "print(f\"Main dataframe shape after dropping geometry: {combined_df.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Final Integrity & Export\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final dataframe shape: (2528, 95)\n",
            "Rows removed due to missing avg_rent_1br: 1\n",
            "Unique neighborhoods in final dataset: 158\n",
            "Year range in final dataset: 2010 - 2024\n",
            "\n",
            "Neighborhood count: 158\n",
            "Expected: ~158 neighborhoods\n",
            "\n",
            "Exported main training file: processed_data/toronto_master_2010_2024.csv\n",
            "File size: 6.13 MB\n",
            "Exported map key file: processed_data/toronto_map_key.csv\n",
            "Map key shape: (158, 4)\n",
            "\n",
            "✅ Pipeline complete!\n"
          ]
        }
      ],
      "source": [
        "# Step 8: Final filtering and export\n",
        "# Remove rows where avg_rent_1br is missing\n",
        "final_df = combined_df[combined_df['avg_rent_1br'].notna()].copy()\n",
        "\n",
        "print(f\"Final dataframe shape: {final_df.shape}\")\n",
        "print(f\"Rows removed due to missing avg_rent_1br: {len(combined_df) - len(final_df)}\")\n",
        "print(f\"Unique neighborhoods in final dataset: {final_df['AREA_NAME'].nunique()}\")\n",
        "print(f\"Year range in final dataset: {final_df['YEAR'].min()} - {final_df['YEAR'].max()}\")\n",
        "\n",
        "# Verify we have all 158 neighborhoods (or close to it)\n",
        "print(f\"\\nNeighborhood count: {final_df['AREA_NAME'].nunique()}\")\n",
        "print(f\"Expected: ~158 neighborhoods\")\n",
        "\n",
        "# Export main training file\n",
        "output_file = PROCESSED_DATA_DIR / 'toronto_master_2010_2024.csv'\n",
        "final_df.to_csv(output_file, index=False)\n",
        "print(f\"\\nExported main training file: {output_file}\")\n",
        "print(f\"File size: {output_file.stat().st_size / 1024 / 1024:.2f} MB\")\n",
        "\n",
        "# Export map key file\n",
        "if not map_key_df.empty:\n",
        "    map_key_file = PROCESSED_DATA_DIR / 'toronto_map_key.csv'\n",
        "    map_key_df.to_csv(map_key_file, index=False)\n",
        "    print(f\"Exported map key file: {map_key_file}\")\n",
        "    print(f\"Map key shape: {map_key_df.shape}\")\n",
        "else:\n",
        "    print(\"Warning: Map key dataframe is empty, skipping export\")\n",
        "\n",
        "print(\"\\n✅ Pipeline complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary Statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== FINAL DATASET SUMMARY ===\n",
            "\n",
            "Shape: (2528, 95)\n",
            "\n",
            "Columns (95):\n",
            "  1. AREA_NAME\n",
            "  2. CLASSIFICATION\n",
            "  3. CLASSIFICATION_CODE\n",
            "  4. Area\n",
            "  5. bachelor_avg_lease_rate\n",
            "  6. 1_bedrooms_leased\n",
            "  7. 1_bed_room_avg_lease_rate\n",
            "  8. 2_bedrooms_leased\n",
            "  9. 2_bedrooms_avg_lease_rate\n",
            "  10. 3_bedrooms_leased\n",
            "  11. 3_bedrooms_avg_lease_rate\n",
            "  12. area_sq_meters\n",
            "  13. perimeter_meters\n",
            "  14. park_count\n",
            "  15. ASSAULT\n",
            "  16. ASSAULT_RATE\n",
            "  17. AUTOTHEFT\n",
            "  18. AUTOTHEFT_RATE\n",
            "  19. BIKETHEFT_RATE\n",
            "  20. BREAKENTER\n",
            "  21. HOMICIDE\n",
            "  22. HOMICIDE_RATE\n",
            "  23. ROBBERY\n",
            "  24. ROBBERY_RATE\n",
            "  25. SHOOTING\n",
            "  26. SHOOTING_RATE\n",
            "  27. THEFTFROMMV\n",
            "  28. THEFTFROMMV_RATE\n",
            "  29. THEFTOVER\n",
            "  30. THEFTOVER_RATE\n",
            "  31. POPULATION\n",
            "  32. total_stop_count\n",
            "  33. avg_stop_frequency\n",
            "  34. max_stop_frequency\n",
            "  35. total_line_length_meters\n",
            "  36. transit_line_density\n",
            "  37. distinct_route_count\n",
            "  38. AreaBachelor Leased\n",
            "  39. Bachelor Leased\n",
            "  40. YEAR\n",
            "  41. avg_rent_1br\n",
            "  42. bachelor_leased_q4\n",
            "  43. bachelor_avg_lease_rate_q4\n",
            "  44. 1_bedroom_leased_q4\n",
            "  45. 1_bedroom_avg_lease_rate_q4\n",
            "  46. 2_bedrooms_leased_q4\n",
            "  47. 2_bedrooms_avg_lease_rate_q4\n",
            "  48. 3_bedrooms_leased_q4\n",
            "  49. 3_bedrooms_avg_lease_rate_q4\n",
            "  50. bachelor_leased_q3\n",
            "  51. bachelor_avg_lease_rate_q3\n",
            "  52. 1_bedroom_leased_q3\n",
            "  53. 1_bedroom_avg_lease_rate_q3\n",
            "  54. 2_bedrooms_leased_q3\n",
            "  55. 2_bedrooms_avg_lease_rate_q3\n",
            "  56. 3_bedrooms_leased_q3\n",
            "  57. 3_bedrooms_avg_lease_rate_q3\n",
            "  58. bachelor_leased_q2\n",
            "  59. bachelor_avg_lease_rate_q2\n",
            "  60. 1_bedroom_leased_q2\n",
            "  61. 1_bedroom_avg_lease_rate_q2\n",
            "  62. 2_bedrooms_leased_q2\n",
            "  63. 2_bedrooms_avg_lease_rate_q2\n",
            "  64. 3_bedrooms_leased_q2\n",
            "  65. 3_bedrooms_avg_lease_rate_q2\n",
            "  66. bachelor_leased_q1\n",
            "  67. bachelor_avg_lease_rate_q1\n",
            "  68. 1_bedroom_leased_q1\n",
            "  69. 1_bedroom_avg_lease_rate_q1\n",
            "  70. 2_bedrooms_leased_q1\n",
            "  71. 2_bedrooms_avg_lease_rate_q1\n",
            "  72. 3_bedrooms_leased_q1\n",
            "  73. 3_bedrooms_avg_lease_rate_q1\n",
            "  74. BIKETHEFT_RATE_2019_1\n",
            "  75. Unnamed: 0\n",
            "  76. BIKETHEFT_RATE_2020_1\n",
            "  77. _id\n",
            "  78. AREA_ID\n",
            "  79. AREA_ATTR_ID\n",
            "  80. PARENT_AREA_ID\n",
            "  81. AREA_SHORT_CODE\n",
            "  82. AREA_LONG_CODE\n",
            "  83. AREA_DESC\n",
            "  84. OBJECTID\n",
            "  85. Region_classif\n",
            "  86. AREA_NAME_1\n",
            "  87. BIKETHEFT\n",
            "  88. BREAKENTER_RATE\n",
            "  89. geometry_wkt_1\n",
            "  90. geometry_type_1\n",
            "  91. BIKETHEFT_RATE_2024_1\n",
            "  92. rent_lag_1\n",
            "  93. rent_lag_2\n",
            "  94. rent_growth_rate\n",
            "  95. TARGET_RENT_5YR\n",
            "\n",
            "\n",
            "Year distribution:\n",
            "YEAR\n",
            "2010    158\n",
            "2011    158\n",
            "2012    158\n",
            "2013    158\n",
            "2014    158\n",
            "2015    158\n",
            "2016    158\n",
            "2017    316\n",
            "2018    158\n",
            "2019    158\n",
            "2020    158\n",
            "2021    158\n",
            "2022    158\n",
            "2023    158\n",
            "2024    158\n",
            "Name: count, dtype: int64\n",
            "\n",
            "\n",
            "Neighborhoods per year:\n",
            "YEAR\n",
            "2010    158\n",
            "2011    158\n",
            "2012    158\n",
            "2013    158\n",
            "2014    158\n",
            "2015    158\n",
            "2016    158\n",
            "2017    158\n",
            "2018    158\n",
            "2019    158\n",
            "2020    158\n",
            "2021    158\n",
            "2022    158\n",
            "2023    158\n",
            "2024    158\n",
            "Name: AREA_NAME, dtype: int64\n",
            "\n",
            "\n",
            "Sample data (first 5 rows):\n",
            "         AREA_NAME                        CLASSIFICATION CLASSIFICATION_CODE  \\\n",
            "0  Agincourt North  Not an NIA or Emerging Neighbourhood                 NaN   \n",
            "1  Agincourt North  Not an NIA or Emerging Neighbourhood                 NaN   \n",
            "2  Agincourt North  Not an NIA or Emerging Neighbourhood                 NaN   \n",
            "3  Agincourt North  Not an NIA or Emerging Neighbourhood                 NaN   \n",
            "4  Agincourt North  Not an NIA or Emerging Neighbourhood                 NaN   \n",
            "\n",
            "          Area  bachelor_avg_lease_rate  1_bedrooms_leased  \\\n",
            "0  Toronto E07                    945.0           8.666667   \n",
            "1  Toronto E07                      NaN          35.333333   \n",
            "2  Toronto E07                      NaN          21.000000   \n",
            "3  Toronto E07                      0.0         133.000000   \n",
            "4  Toronto E07                      0.0         135.000000   \n",
            "\n",
            "   1_bed_room_avg_lease_rate  2_bedrooms_leased  2_bedrooms_avg_lease_rate  \\\n",
            "0                1113.333333          16.333333                1407.000000   \n",
            "1                1224.666667          39.666667                1424.666667   \n",
            "2                1272.750000          29.500000                1451.250000   \n",
            "3                1318.250000         179.000000                1484.250000   \n",
            "4                1301.500000         157.000000                1518.500000   \n",
            "\n",
            "   3_bedrooms_leased  ...  AREA_NAME_1  BIKETHEFT  BREAKENTER_RATE  \\\n",
            "0                2.0  ...          NaN        NaN              NaN   \n",
            "1                9.0  ...          NaN        NaN              NaN   \n",
            "2                3.5  ...          NaN        NaN              NaN   \n",
            "3               12.0  ...          NaN        NaN              NaN   \n",
            "4               10.0  ...          NaN        NaN              NaN   \n",
            "\n",
            "   geometry_wkt_1  geometry_type_1  BIKETHEFT_RATE_2024_1   rent_lag_1  \\\n",
            "0             NaN              NaN                    NaN          NaN   \n",
            "1             NaN              NaN                    NaN  1113.333333   \n",
            "2             NaN              NaN                    NaN  1224.666667   \n",
            "3             NaN              NaN                    NaN  1272.750000   \n",
            "4             NaN              NaN                    NaN  1318.250000   \n",
            "\n",
            "    rent_lag_2  rent_growth_rate  TARGET_RENT_5YR  \n",
            "0          NaN               NaN      1342.250000  \n",
            "1          NaN         10.000000      1492.166667  \n",
            "2  1113.333333          3.926238      1644.833333  \n",
            "3  1224.666667          3.574936      1801.666667  \n",
            "4  1272.750000         -1.270624      1868.265625  \n",
            "\n",
            "[5 rows x 95 columns]\n",
            "\n",
            "\n",
            "Missing values:\n",
            "CLASSIFICATION_CODE          1840\n",
            "bachelor_avg_lease_rate       737\n",
            "1_bed_room_avg_lease_rate    1106\n",
            "2_bedrooms_leased            1106\n",
            "2_bedrooms_avg_lease_rate      17\n",
            "                             ... \n",
            "BIKETHEFT_RATE_2024_1        2370\n",
            "rent_lag_1                    158\n",
            "rent_lag_2                    316\n",
            "rent_growth_rate              158\n",
            "TARGET_RENT_5YR               790\n",
            "Length: 80, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Display summary statistics\n",
        "print(\"=== FINAL DATASET SUMMARY ===\")\n",
        "print(f\"\\nShape: {final_df.shape}\")\n",
        "print(f\"\\nColumns ({len(final_df.columns)}):\")\n",
        "for i, col in enumerate(final_df.columns, 1):\n",
        "    print(f\"  {i}. {col}\")\n",
        "\n",
        "print(f\"\\n\\nYear distribution:\")\n",
        "print(final_df['YEAR'].value_counts().sort_index())\n",
        "\n",
        "print(f\"\\n\\nNeighborhoods per year:\")\n",
        "print(final_df.groupby('YEAR')['AREA_NAME'].nunique())\n",
        "\n",
        "print(f\"\\n\\nSample data (first 5 rows):\")\n",
        "print(final_df.head())\n",
        "\n",
        "print(f\"\\n\\nMissing values:\")\n",
        "missing = final_df.isnull().sum()\n",
        "print(missing[missing > 0])\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
